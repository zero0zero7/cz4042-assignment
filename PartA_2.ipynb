{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c7e82aadc4d77a8b23f7f880449f9e3",
     "grade": false,
     "grade_id": "a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Question A2 (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb28aa752ce5540f5b18d10694b52ea9",
     "grade": false,
     "grade_id": "a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation.\n",
    "\n",
    "* note: some cells are non-editable and cannot be filled, but leave them untouched. Fill up only cells which are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aceec82011f43733c0551ca196f1b16c",
     "grade": false,
     "grade_id": "a2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {128, 256, 512, 1024}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection.\n",
    "\n",
    "This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0edc610-21e6-4cc7-9603-59318b961990",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b0edc610-21e6-4cc7-9603-59318b961990",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "909acb3c7ff3883eb5381eb586615d3b",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "from statistics import mean \n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e12861-4713-4914-9f4b-8a7381708243",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8e12861-4713-4914-9f4b-8a7381708243",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed97d9f30da032a5e349047c614efec1",
     "grade": false,
     "grade_id": "a2_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2. To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "\n",
    "in a separate file called **common_utils.py**\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1a982-de85-46de-b890-3b81f79f5887",
   "metadata": {
    "deletable": false,
    "id": "37a1a982-de85-46de-b890-3b81f79f5887",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9db3ca972642b1447dba3ebd5f2db24b",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import MLP, CustomDataset, loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6b33318200b4bc38d431576963edb1",
     "grade": true,
     "grade_id": "correct_import",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c738d3b4888de90dda8c532036bc5fe5",
     "grade": false,
     "grade_id": "a2_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3. Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
   "metadata": {
    "deletable": false,
    "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d02dac62baa528c191eb4f47b2495406",
     "grade": false,
     "grade_id": "dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_utils import split_dataset, preprocess_dataset\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "df_train, y_train, df_test, y_test = split_dataset(df, ['filename'], 0.3, 42)\n",
    "# Remove 'label' column from df_train and df_test to get X_train and X_test\n",
    "X_train = df_train.drop(columns=[\"label\"])\n",
    "X_test = df_test.drop(columns=[\"label\"])\n",
    "\n",
    "def generate_cv_folds_for_batch_sizes(parameters, X_train, y_train):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_train_scaled_dict(dict) where X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds.\n",
    "    X_val_scaled_dict(dict) where X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds.\n",
    "    y_train_dict(dict) where y_train_dict[batch_size] is a list of labels for the different folds\n",
    "    y_val_dict(dict) where y_val_dict[batch_size] is a list of labels for the different folds\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = {}, {}, {}, {}\n",
    "    for batch_size in parameters:\n",
    "        for train_index, val_index in kf.split(X_train, y_train):\n",
    "            # -> train_index is a list of indices for the training set, val_index is a list of indices for the validation set\n",
    "            y_train_dict[batch_size], y_val_dict[batch_size] = y_train[train_index], y_train[val_index]\n",
    "            # Scale the X training set and X validation set\n",
    "            standard_scaler = preprocessing.StandardScaler()\n",
    "            X_train_, X_val_ = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            # Only fit the scaler to training data, not to validation nor test data\n",
    "            X_train_scaled = standard_scaler.fit_transform(X_train_) # X_train_scaled is now np.ndarray\n",
    "            X_val_scaled = standard_scaler.transform(X_val_) # X_val_scaled is now np.ndarray\n",
    "            X_train_scaled_dict[batch_size] = X_train_scaled\n",
    "            X_val_scaled_dict[batch_size] = X_val_scaled\n",
    "    return X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict\n",
    "\n",
    "batch_sizes = [128, 256, 512, 1024]\n",
    "X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = generate_cv_folds_for_batch_sizes(batch_sizes, X_train, y_train)\n",
    "# -> len(X_train_scaled_dict[batch_size]) = 33756 ie. 8439 * 4\n",
    "# -> len(X_val_scaled_dict[batch_size]) = 8439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ca332-9676-42bd-9801-0f5f4157a777",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "235ca332-9676-42bd-9801-0f5f4157a777",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae5f281cd84f4d36f81f2ae126cf915",
     "grade": true,
     "grade_id": "correct_dataset",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dcf6be1ad49306172e6f27243e613f2",
     "grade": true,
     "grade_id": "correct_dataset2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "064d68c9708b5e3f1e2463001b6d78b4",
     "grade": false,
     "grade_id": "a2_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "4. Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
   "metadata": {
    "deletable": false,
    "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9665887943f38ae7bed6c1d8351903b",
     "grade": true,
     "grade_id": "hyperparameter_tuning",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from common_utils import EarlyStopper\n",
    "no_epochs = 100\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def train_val(model, loss_fn, optimizer, early_stopper, val_accuracies, val_losses, training_times, batch_size, training_dataloader, validation_dataloader):\n",
    "\tfor epoch in range(no_epochs):\n",
    "\t\t\t# Training\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\tfor i, (x, y) in enumerate(training_dataloader):\n",
    "\t\t\t\t# Compute prediction and Loss\n",
    "\t\t\t\ty_pred = model(x) # shape: [256, 1]\n",
    "\t\t\t\ty_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "\t\t\t\tloss = loss_fn(y_pred.float(), y.float())\n",
    "\t\t\t\t# Backpropagation\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\ttraining_time = time.time() - start_time\n",
    "\t\t\ttraining_times[batch_size] = training_time\n",
    "\t\t\t# Validation\n",
    "\t\t\tval_acc, val_loss = 0, 0\n",
    "\t\t\tno_input = len(validation_dataloader.dataset)\n",
    "\t\t\tno_batch = len(validation_dataloader)\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tfor i, (x, y) in enumerate(validation_dataloader):\n",
    "\t\t\t\t\t# Prediction\n",
    "\t\t\t\t\ty_pred = model(x) # shape: [256, 1]\n",
    "\t\t\t\t\ty_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "\t\t\t\t\t# Compute loss and accuracy\n",
    "\t\t\t\t\tloss = loss_fn(y_pred.float(), y.float())\n",
    "\t\t\t\t\tval_loss += loss.item()\n",
    "\t\t\t\t\t# transform y_pred to give class 0 or class 1\n",
    "\t\t\t\t\tpred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\t\t\t\t\t# compare pred_label with the ground truth y, add 1 to the accuracy variable if the prediction is correct\n",
    "\t\t\t\t\tval_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "\t\t\tval_loss /= no_batch\n",
    "\t\t\tval_acc /= no_input\n",
    "\t\t\tval_accuracies.append(val_acc)\n",
    "\t\t\tval_losses.append(val_loss)\n",
    "\t\t\tif early_stopper.early_stop(val_loss):\n",
    "\t\t\t\tprint(\"Batch size: \", batch_size)\n",
    "\t\t\t\tprint(\"Early stopping at epoch number: \", epoch+1, \"validation accuracy: \", val_acc, '\\n')\n",
    "\t\t\t\tbreak\n",
    "\treturn val_accuracies, val_losses, training_times\n",
    "\n",
    "def find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, mode, batch_sizes=[128, 256, 512, 1024], no_epoch=100):\n",
    "\tif mode != 'batch_size':\n",
    "\t\tprint(\"Invalid mode\")\n",
    "\t\treturn\n",
    "\tmean_cross_validation_accuracies, training_times = {}, {}\n",
    "\tfor batch_size in batch_sizes:\n",
    "\t\tmodel = MLP(no_hidden=128, no_features=77, no_labels=1)\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\t\tloss_fn = nn.BCELoss()\n",
    "\t\tearly_stopper = EarlyStopper()\n",
    "\t\t# Load Data\n",
    "\t\ttraining_data = CustomDataset(X_train_scaled_dict[batch_size], y_train_dict[batch_size])\n",
    "\t\ttraining_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\t\tvalidation_data = CustomDataset(X_val_scaled_dict[batch_size], y_val_dict[batch_size])\n",
    "\t\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\t\t# Train and Validate\n",
    "\t\tval_accuracies, val_losses = [], []\n",
    "\t\tval_accuracies, val_losses, training_times = train_val(model, loss_fn, optimizer, early_stopper, val_accuracies, val_losses, training_times, batch_size, training_dataloader, validation_dataloader)\n",
    "\t\tmean_cross_validation_accuracies[batch_size] = mean(val_accuracies)\n",
    "\treturn mean_cross_validation_accuracies, training_times\n",
    "\n",
    "batch_sizes = [128, 256, 512, 1024]\n",
    "mean_cross_validation_accuracies, training_times = find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, 'batch_size', batch_sizes, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975e552e751c4efb2cec0eac214f85cd",
     "grade": true,
     "grade_id": "correct_hyperparameter_tuning",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69421943e22521de848bb03a50f57767",
     "grade": false,
     "grade_id": "a2_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "5. Plot scatterplot of mean cross validation accuracies for the different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17599eb29fd6e3a1e2812f0ff7cba983",
     "grade": true,
     "grade_id": "plot",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def acc_batchsize(cross_validation_accuracies):\n",
    "\tplt.figure(1)\n",
    "\tplt.ylabel(\"mean cross validation accuracy\")\n",
    "\tplt.xticks(list(mean_cross_validation_accuracies.keys()))\n",
    "\tplt.xlabel(\"batch size\")\n",
    "\tplt.scatter(cross_validation_accuracies.keys(), cross_validation_accuracies.values())\n",
    "\tplt.show()\n",
    "\n",
    "acc_batchsize(mean_cross_validation_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11e8d298b5774c4044f1c3f950c46214",
     "grade": false,
     "grade_id": "a2_1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "6. Create a table of time taken to train the network on the last epoch against different batch sizes. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c18e30a9850c282ad725336848222a62",
     "grade": false,
     "grade_id": "times",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Batch Size': [128, 256, 512, 1024],\n",
    "                   'Last Epoch Time': [training_times[128], training_times[256], training_times[512], training_times[1024]]\n",
    "                  })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2fc4a52c2a0af7ea586ea85cec9b3e9",
     "grade": true,
     "grade_id": "correct_times",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
   "metadata": {
    "deletable": false,
    "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38690f32ec506325fc73c8353b77d041",
     "grade": false,
     "grade_id": "batch_size",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "optimal_batch_size = 256\n",
    "reason = \"It achieves the highest mean cross validation accuracy. While it does take a slightly longer time than 512 and 1024, it is more reasonable to prioritise accuracy because the training time for each epoch only takes 0.097553 seconds and from the above output, when early stopping is implemented only 21 epochs are needed. Hence, the total training time is less of a concern as compared to the accuracy of the model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f695b961ed43ec6a31b7647e078fd8d6",
     "grade": true,
     "grade_id": "correct_batch_size",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
