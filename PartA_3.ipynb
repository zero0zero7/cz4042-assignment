{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e726e0bc51bda6101498fa65e298d55",
     "grade": false,
     "grade_id": "cell-a5df181492bc4d5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f824c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5c8f824c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8de5fc7baf0dbbe0ae44ff5e2f33d3e2",
     "grade": false,
     "grade_id": "cell-742f6ec36e67f66e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Find the optimal number of hidden neurons for first hidden layer of the 4-layer network (3 hidden layers, output layer) designed in Question 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c699e75b7edb52e94f6605279070c695",
     "grade": false,
     "grade_id": "cell-e96803fd0366edd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Plot the mean cross-validation accuracies on the final epoch for different numbers of hidden-layer neurons using a scatter plot. Limit the search space of the number of neurons to {64, 128, 256}. Continue using 5-fold cross validation on training dataset. Select the optimal number of neurons for the hidden layer. State the rationale for your selection.\n",
    "\n",
    "This might take a while to run, approximately 20 - 30 min, so plan your time carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c85ca-9a14-4d0a-b44d-814f02c0f8e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "981c85ca-9a14-4d0a-b44d-814f02c0f8e1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "227f476ef461a471ef38af7d3f6715f8",
     "grade": false,
     "grade_id": "cell-808458412f82c806",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Firstly, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0edc610-21e6-4cc7-9603-59318b961990",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b0edc610-21e6-4cc7-9603-59318b961990",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b3385379104c8466a67ea59116fe58",
     "grade": false,
     "grade_id": "cell-03d073049be6df79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from common_utils import set_seed\n",
    "\n",
    "import time\n",
    "from common_utils import EarlyStopper\n",
    "from common_utils import split_dataset, preprocess_dataset\n",
    "from statistics import mean\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e12861-4713-4914-9f4b-8a7381708243",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8e12861-4713-4914-9f4b-8a7381708243",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1203c0efa2d363ccc72779a7511ed5b5",
     "grade": false,
     "grade_id": "cell-647b74152d4edf45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "\n",
    "in a separate file called **common_utils.py**\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1a982-de85-46de-b890-3b81f79f5887",
   "metadata": {
    "deletable": false,
    "id": "37a1a982-de85-46de-b890-3b81f79f5887",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52ed938922eb6062a33a7d047d8fc605",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import MLP, CustomDataset, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0e215-033b-4720-89f8-64f96574ebe2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "88f0e215-033b-4720-89f8-64f96574ebe2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6b33318200b4bc38d431576963edb1",
     "grade": true,
     "grade_id": "correct_import",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79004bd568c9f48abd1cf359cd050ab5",
     "grade": false,
     "grade_id": "cell-10b7165b0a25758f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Perform hyperparameter tuning for the different neurons with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77bd18-c546-473e-8c2f-643b4281d9ba",
   "metadata": {
    "deletable": false,
    "id": "8c77bd18-c546-473e-8c2f-643b4281d9ba",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c610a779f6858f6c77f3fc6beb198dcd",
     "grade": true,
     "grade_id": "train",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, X_train_scaled, y_train2, X_val_scaled, y_val2, batch_size):\n",
    "    # YOUR CODE HERE\n",
    "    train_accuracies, train_losses, test_accuracies, test_losses, times = [], [], [], [], []\n",
    "    # each list contains the accuracy/loss/time for each epoch \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopper = EarlyStopper()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    # Load data\n",
    "    training_data = CustomDataset(X_train_scaled, y_train2)\n",
    "    training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    validation_data = CustomDataset(X_val_scaled, y_val2)\n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "    no_epochs = 100\n",
    "    for epoch in range(no_epochs):\n",
    "        # Training\n",
    "        start_time = time.time()\n",
    "        train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "        for i, (x, y) in enumerate(training_dataloader):\n",
    "            # Prediction\n",
    "            y_pred = model(x) # shape: [256, 1]\n",
    "            y_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred.float(), y.float())\n",
    "            train_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            # transform y_pred to give class 0 or class 1\n",
    "            pred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "            # compare pred_label with the ground truth y, add 1 to the train_acc variable if the prediction is correct\n",
    "            train_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        training_time = time.time() - start_time\n",
    "        times.append(training_time)\n",
    "        # To get mean loss per batch, divide by len(validation_dataloader) ie. number of batches\n",
    "        train_losses.append(train_loss/ len(training_dataloader))\n",
    "        # To get mean accuracy per input, divide by len(validation_dataloader.dataset) ie. number of inputs\n",
    "        train_accuracies.append(train_acc/ len(training_dataloader.dataset))\n",
    "        # Testing\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(validation_dataloader):\n",
    "                # Prediction\n",
    "                y_pred = model(x) # shape: [256, 1]\n",
    "                y_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "                # Compute loss\n",
    "                loss = loss_fn(y_pred.float(), y.float())\n",
    "                test_loss += loss.item()\n",
    "                # Compute accuracy\n",
    "                # transform y_pred to give class 0 or class 1\n",
    "                pred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "                # compare pred_label with the ground truth y, add 1 to the test_acc variable if the prediction is correct\n",
    "                test_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "        # To get mean loss per batch, divide by len(validation_dataloader)\n",
    "        test_losses.append(test_loss/ len(validation_dataloader))\n",
    "        # To get mean accuracy per input, divide by len(validation_dataloader.dataset)\n",
    "        test_accuracies.append(test_acc/ len(validation_dataloader.dataset))\n",
    "        if early_stopper.early_stop(test_loss):\n",
    "            print(\"Early stopping at epoch number: \", epoch+1)\n",
    "            break\n",
    "    return train_accuracies, train_losses, test_accuracies, test_losses, times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d1ff1-7242-4c3f-bcf2-c92fe0c723db",
   "metadata": {
    "deletable": false,
    "id": "d13d1ff1-7242-4c3f-bcf2-c92fe0c723db",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec4e70e2dfdc84af8c83bc858117af1f",
     "grade": true,
     "grade_id": "hyperparameter_tuning",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_optimal_hyperparameter(X_train, y_train, parameters, mode, batch_size):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if mode != \"num_neurons\":\n",
    "        print(\"Please enter a valid mode\")\n",
    "        return\n",
    "    cross_validation_accuracies_final_epoch, training_times = {}, {}\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for parameter in parameters:\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "            # Scale the training set and validation set separately\n",
    "            standard_scaler = preprocessing.StandardScaler()\n",
    "            X_train_, X_val_ = X_train[train_index], X_train[val_index]\n",
    "            # Only fit the scaler to training data, not to validation nor test data\n",
    "            X_train_scaled = standard_scaler.fit_transform(X_train_) \n",
    "            X_val_scaled = standard_scaler.transform(X_val_)\n",
    "            y_train2 = y_train[train_index]\n",
    "            y_val2 = y_train[val_index]\n",
    "        model = MLP(no_hidden=parameter, no_features=77, no_labels=1)\n",
    "        train_accuracies, train_losses, test_accuracies, test_losses, times = train(model, X_train_scaled, y_train2, X_val_scaled, y_val2, batch_size)\n",
    "        cross_validation_accuracies_final_epoch[parameter] = test_accuracies[-1]\n",
    "        training_times[parameter] = mean(times)\n",
    "\n",
    "        print(\"Parameter: \", parameter)\n",
    "        print(\"Mean cross-validation accuracy on last epoch: \", cross_validation_accuracies_final_epoch[parameter])\n",
    "        print(\"Mean training time: \", training_times[parameter])\n",
    "        \n",
    "    return cross_validation_accuracies_final_epoch, training_times\n",
    "\n",
    "'''\n",
    "optimal_bs = 0. Fill your optimal batch size in the following code.\n",
    "'''\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "df_train, y_train, df_test, y_test = split_dataset(df, ['filename'], 0.3, 42)\n",
    "X_train = df_train.drop(columns=[\"label\"])\n",
    "\n",
    "num_neurons = [64, 128, 256]\n",
    "# use optimal batch size of 256 found in A2\n",
    "cross_validation_accuracies, cross_validation_times = find_optimal_hyperparameter(X_train.to_numpy(), y_train, num_neurons, 'num_neurons', batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4cce9-3a65-47fa-9b34-44b75de57dff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "13b4cce9-3a65-47fa-9b34-44b75de57dff",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975e552e751c4efb2cec0eac214f85cd",
     "grade": true,
     "grade_id": "correct_hyperparameter_tuning",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0de56ab3a8b732e3ada17c55bc90a3cf",
     "grade": false,
     "grade_id": "cell-d0eceff23b1291e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Plot the cross-validation accuracies against the number of epochs for different numbers of hidden-layer neurons. Limit the search space of the number of neurons to {64, 128, 256}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25582e96a2b85a5ce6e0cf48a58064bd",
     "grade": true,
     "grade_id": "cell-plot",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def acc_neuron(cross_validation_accuracies):\n",
    "\tplt.figure(1)\n",
    "\tplt.ylabel(\"mean cross-validation accuracy on last epoch\")\n",
    "\tplt.xlabel(\"number of neurons\")\n",
    "\tx = cross_validation_accuracies.keys()\n",
    "\ty = cross_validation_accuracies.values()\n",
    "\tplt.xticks(list(x))\n",
    "\tplt.scatter(x, y)\n",
    "\tplt.show()\n",
    "\n",
    "acc_neuron(cross_validation_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2024917155ea0cc8755c69415b4956cf",
     "grade": false,
     "grade_id": "part-1-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Select the optimal number of neurons for the hidden layer. State the rationale for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45a22d-9f20-441b-986b-dca35083abc3",
   "metadata": {
    "deletable": false,
    "id": "3e45a22d-9f20-441b-986b-dca35083abc3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f1233c3ecacc38527a60e87185938b",
     "grade": false,
     "grade_id": "reason",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_neurons = 128\n",
    "reason = \"Highest mean cross valiation accuracy. It takes the longest time to train (0.68 seconds per epoch), but the difference is very small compared to the other two neuron numbers is only 0.03seconds. Hence, accuracy takes the priority.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a76852-5fd9-48e6-b6ee-d0b9277340fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e9a76852-5fd9-48e6-b6ee-d0b9277340fa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8632aa5f8381d571f8992f3196e7aab",
     "grade": true,
     "grade_id": "reason1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebf2dc5e-91d5-49dc-a05f-b9318f3371a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ebf2dc5e-91d5-49dc-a05f-b9318f3371a7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c609ceb4ecc2dac0684b1da17f44daa",
     "grade": false,
     "grade_id": "cell-302503e166f647c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Plot the train and test accuracies against training epochs with the optimal number of neurons using a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2776dd57325ada98de4534313e894572",
     "grade": true,
     "grade_id": "figure",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import preprocess_dataset\n",
    "\n",
    "def train_test(X_train_scaled, y_train, X_test_scaled, y_test, batch_size=256):\n",
    "\ttrain_accuracies, test_accuracies = [], []\n",
    "\t# Use 128 neurons as found above\n",
    "\tmodel = MLP(no_hidden=128, no_features=77, no_labels=1)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\tloss_fn = nn.BCELoss()\n",
    "\tearly_stopper = EarlyStopper()\n",
    "\ttraining_data = CustomDataset(X_train_scaled, y_train)\n",
    "\ttraining_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\ttest_data = CustomDataset(X_test_scaled, y_test)\n",
    "\ttest_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\tno_epochs = 100\n",
    "\tfor epoch in range(no_epochs):\n",
    "\t\ttrain_acc, test_acc = 0, 0\n",
    "\t\t# Training\n",
    "\t\tfor i, (x, y) in enumerate(training_dataloader):\n",
    "\t\t\t# Prediction\n",
    "\t\t\ty_pred = model(x) # shape: [256, 1]\n",
    "\t\t\ty_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "\t\t\t# Compute loss\n",
    "\t\t\tloss = loss_fn(y_pred.float(), y.float())\n",
    "\t\t\t# Compute accuracy\n",
    "\t\t\t# transform y_pred to give class 0 or class 1\n",
    "\t\t\tpred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\t\t\t# compare pred_label with the ground truth y, add 1 to the train_acc variable if the prediction is correct\n",
    "\t\t\ttrain_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "\t\t\t# Backpropagation\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t# To get mean accuracy per input, divide by len(validation_dataloader.dataset) ie. the number of inputs\n",
    "\t\ttrain_accuracies.append(train_acc/ len(training_dataloader.dataset))\n",
    "\t\t# Testing\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i, (x, y) in enumerate(test_dataloader):\n",
    "\t\t\t\t# Prediction\n",
    "\t\t\t\ty_pred = model(x) # shape: [256, 1]\n",
    "\t\t\t\ty_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "\t\t\t\t# Compute accuracy\n",
    "\t\t\t\t# transform y_pred to give class 0 or class 1\n",
    "\t\t\t\tpred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\t\t\t\t# compare pred_label with the ground truth y, add 1 to the test_acc variable if the prediction is correct\n",
    "\t\t\t\ttest_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "\t\t# To get mean accuracy per input, divide by len(validation_dataloader.dataset) ie. the number of inputs\n",
    "\t\ttest_accuracies.append(test_acc/ len(test_dataloader.dataset))\n",
    "\treturn train_accuracies, test_accuracies\n",
    "\n",
    "\t\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "df_train, y_train, df_test, y_test = split_dataset(df, ['filename'], 0.3, 42)\n",
    "X_train = df_train.drop(columns=[\"label\"])\n",
    "X_test = df_test.drop(columns=[\"label\"])\n",
    "X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)\n",
    "\n",
    "# Use optimal batch size of 256 as found in part A2\n",
    "train_acc_list, test_acc_list = train_test(X_train_scaled, y_train, X_test_scaled, y_test, batch_size=256)\n",
    "\n",
    "def acc_epoch(train_acc_list, test_acc_list):\n",
    "\t\tplt.figure(1)\n",
    "\t\tplt.ylabel(\"accuracy\")\n",
    "\t\tplt.xlabel(\"epoch number\")\n",
    "\t\tplt.plot(range(len(train_acc_list)), train_acc_list, label=\"train accuracy\")\n",
    "\t\tplt.plot(range(len(train_acc_list)), test_acc_list, label=\"test accuracy\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.show()\n",
    "\n",
    "acc_epoch(train_acc_list, test_acc_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
