{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDDK5s1_mFRg"
   },
   "source": [
    "CS4001/4042 Assignment 1, Part B, Q3\n",
    "---\n",
    "\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7WFI5tMpqGc"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRCFpMEd3w8W"
   },
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utC2haR03sQY"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUU-C3eRmWeE"
   },
   "source": [
    "> First, load the dataset following the splits in Question B1. To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Do not standardise the numerical features for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNtpumjamL1N"
   },
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "numeric_df = df.drop(columns=['town', 'month', 'full_address', 'nearest_stn', 'storey_range', 'flat_model_type', ])\n",
    "\n",
    "train_df = numeric_df[numeric_df['year'] <= 2019]\n",
    "train_df = train_df.drop(columns=['year'])\n",
    "val_df = numeric_df[numeric_df['year'] == 2020]\n",
    "val_df = val_df.drop(columns=['year'])\n",
    "test_df = numeric_df[numeric_df['year'] == 2021]\n",
    "test_df = test_df.drop(columns=['year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4L7QdqLmX2s"
   },
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Saliency, Input x Gradients, Integrated Gradients, GradientSHAP, Feature Ablation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGIWUq9Fmct8"
   },
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "# Prepare data\n",
    "X_train = torch.from_numpy(train_df.drop(columns=['resale_price']).to_numpy()).float() # torch.Size([64057, 6])\n",
    "y_train = torch.tensor(train_df['resale_price'].values).view(-1, 1).float() # torch.Size([64057, 1])\n",
    "\n",
    "X_test = torch.from_numpy(test_df.drop(columns=['resale_price']).to_numpy()).float() # torch.Size([29057, 6])\n",
    "y_test = torch.tensor(test_df['resale_price'].values).view(-1, 1).float() # torch.Size([29057, 1])\n",
    "\n",
    "\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size=10, shuffle=True)\n",
    "\n",
    "# Set default hyperparams\n",
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "size_hidden1 = 5\n",
    "size_hidden2 = 5\n",
    "size_hidden3 = 5\n",
    "size_output = 1\n",
    "\n",
    "# Build model\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Only 6 input features after dropping categorical features and the target feature 'resale_price'\n",
    "        self.lin1 = nn.Linear(6, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.lin4 = nn.Linear(size_hidden3, size_output)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin4(self.relu3(self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input)))))))\n",
    "\n",
    "# Train model\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "def train(model_inp, train_iter, num_epochs):\n",
    "    optimizer = torch.optim.Adam(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0 # for each epoch\n",
    "        for inputs, labels in train_iter:\n",
    "            # forward pass\n",
    "            outputs = model_inp(inputs)\n",
    "            # defining loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # computing gradients\n",
    "            loss.backward()\n",
    "            # accumulating running loss\n",
    "            running_loss += loss.item()\n",
    "            # updated weights based on computed gradients\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:    \n",
    "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                  (epoch + 1, num_epochs, running_loss))\n",
    "        running_loss = 0.0\n",
    "    return model_inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myModel()\n",
    "model.train()\n",
    "model = train(model, train_iter, num_epochs)\n",
    "\n",
    "# Save model\n",
    "torch.save(model, 'unscaled_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "\n",
    "print('model rsme: ', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributions\n",
    "def saliency(model, test):\n",
    "\tsal = Saliency(model)\n",
    "\tsal_attr_test = sal.attribute(test)\n",
    "\treturn sal_attr_test\n",
    "\n",
    "def\tinput_x_gradient(model, test):\n",
    "\tixg = InputXGradient(model)\n",
    "\tixg_attr_test = ixg.attribute(test)\n",
    "\treturn ixg_attr_test\n",
    "\n",
    "def integrated_gradients(model, test):\n",
    "\tig = IntegratedGradients(model)\n",
    "\tig_attr_test = ig.attribute(test, n_steps=50)\n",
    "\treturn ig_attr_test\n",
    "\n",
    "def gradient_shap(model, test, train):\n",
    "\tgs = GradientShap(model)\n",
    "\t# use the entire training dataset as the distribution of baselines\n",
    "\tgs_attr_test = gs.attribute(test, train)\n",
    "\treturn gs_attr_test\n",
    "\n",
    "def feature_ablation(model, test):\n",
    "\tfa = FeatureAblation(model)\n",
    "\tfa_attr_test = fa.attribute(test)\n",
    "\treturn fa_attr_test\n",
    "\n",
    "def attr_test_norm_sum(attr_test):\n",
    "\tattr_test_sum = attr_test.detach().numpy().sum(0)\n",
    "\tattr_test_norm_sum = attr_test_sum / np.linalg.norm(attr_test_sum, ord=1)\n",
    "\treturn attr_test_norm_sum\n",
    "\n",
    "def get_y_axis_lin_weight(model):\n",
    "\tlin_weight = model.lin1.weight[0].detach().numpy()\n",
    "\ty_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n",
    "\treturn y_axis_lin_weight\n",
    "\n",
    "# Plottings\n",
    "def plot(x_test, attr_test_norm_sum, y_axis_lin_weight, title='Comparing input feature importances across multiple algorithms and learned weights', legends=['Saliency', 'Input X Gradients', 'Integrated Gradients', 'GradientSHAP', 'Feature Ablation', 'Weights']):\n",
    "\twidth = 0.14\n",
    "\tplt.figure(figsize=(20, 10))\n",
    "\tax = plt.subplot()\n",
    "\tax.set_title(title)\n",
    "\tax.set_ylabel('Attributions')\n",
    "\tFONT_SIZE = 16\n",
    "\tplt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n",
    "\tplt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n",
    "\tplt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n",
    "\tplt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "\tx_axis_data = np.arange(x_test.shape[1])\n",
    "\tfeature_names = list(train_df.drop(columns=['resale_price']).columns)\n",
    "\tx_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "\ti = 1\n",
    "\tcolours = ['#A90000', '#34b8e0', '#eb5e7c', '#4260f5', '#49ba81', 'grey']\n",
    "\twhile i <= len(attr_test_norm_sum):\n",
    "\t\tax.bar(x_axis_data + i * width, attr_test_norm_sum[i-1], width, align='center', alpha=0.75, color=colours[i-1])\n",
    "\t\ti += 1\n",
    "\tax.bar(x_axis_data + i * width, y_axis_lin_weight, width, align='center', alpha=1.0, color='grey')\n",
    "\n",
    "\tax.autoscale_view()\n",
    "\tplt.tight_layout()\n",
    "\tax.set_xticks(x_axis_data + 0.5)\n",
    "\tax.set_xticklabels(x_axis_data_labels)\n",
    "\tax.grid(axis='y')\n",
    "\tplt.legend(legends, loc=3)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the attribution calculations, we will limit the test input size to 5000\n",
    "\n",
    "# X_test_5000 = torch.empty((5000, 6), dtype=torch.float32)\n",
    "# for idx in range(5000):\n",
    "# \ti = random.randint(0, X_test.shape[0]-1)\n",
    "# \tX_test_5000[idx] = X_test[i]\n",
    "\n",
    "X_test_5000 = X_test[:1000]\n",
    "\n",
    "sal_attr_test = saliency(model, X_test_5000)\n",
    "ixg_attr_test = input_x_gradient(model, X_test_5000)\n",
    "ig_attr_test = integrated_gradients(model, X_test_5000)\n",
    "gs_attr_test = gradient_shap(model, X_test_5000, X_train)\n",
    "fa_attr_test = feature_ablation(model, X_test_5000)\n",
    "\n",
    "attr_test_norm_sum_list = []\n",
    "for attr_test in [sal_attr_test, ixg_attr_test, ig_attr_test, gs_attr_test, fa_attr_test]:\n",
    "\tattr_test_norm_sum_list.append(attr_test_norm_sum(attr_test))\n",
    "\n",
    "y_axis_lin_weight = get_y_axis_lin_weight(model)\n",
    "\n",
    "plot(X_test_5000, attr_test_norm_sum_list, y_axis_lin_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DexR-OzAmd26"
   },
   "source": [
    "> Train a separate model with the same configuration but now standardise the features via **StandardScaler** (fit to training set, then transform all). State your observations with respect to GradientShap and explain why it has occurred.\n",
    "(Hint: Many gradient-based approaches depend on a baseline, which is an important choice to be made. Check the default baseline settings carefully.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzRk02TnmgyB"
   },
   "outputs": [],
   "source": [
    "y_train_df = train_df['resale_price'] # (64057,)\n",
    "torch.tensor(y_train_df.values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "# Scale data\n",
    "X_train_df = train_df.drop(columns=['resale_price']) # (64057, 6) \n",
    "X_test_df = test_df.drop(columns=['resale_price']) # (29057, 6)\n",
    "y_train_df = train_df['resale_price'] # (64057,)\n",
    "y_test_df = test_df['resale_price'] # (29057,)\n",
    "\n",
    "x_standard_scaler = preprocessing.StandardScaler()\n",
    "# y_standard_scaler = preprocessing.StandardScaler()\n",
    "x_standard_scaler.fit(X_train_df)\n",
    "# y_standard_scaler.fit(torch.tensor(y_train_df.values).view(-1, 1))\n",
    "# torch.tensor(y_train_df.values).view(-1, 1).shape -> torch.Size([64057, 1])\n",
    "\n",
    "x_train_scaled = x_standard_scaler.transform(X_train_df)\n",
    "X_train_scaled_tensor = torch.tensor(x_train_scaled, dtype=torch.float32) #torch.Size([64057, 6]) \n",
    "# y_train_scaled = y_standard_scaler.transform(y_train_df.values.reshape(-1, 1))\n",
    "# y_train_scaled_tensor = torch.tensor(y_train_scaled).float() #torch.Size([64057, 1])\n",
    "x_test_scaled = x_standard_scaler.transform(X_test_df)\n",
    "X_test_scaled_tensor = torch.tensor(x_test_scaled, dtype=torch.float32) #torch.Size([29057, 6]) \n",
    "# y_test_scaled = y_standard_scaler.transform(y_test_df.values.reshape(-1, 1))\n",
    "# y_test_scaled_tensor = torch.tensor(y_test_scaled).float() #torch.Size([29057, 1])\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_df.values, dtype=torch.float32) #torch.Size([64057])\n",
    "y_test_tensor = torch.tensor(y_test_df.values, dtype=torch.float32) #torch.Size([29057])\n",
    "\n",
    "\n",
    "# Load data\n",
    "datasets_scaled = torch.utils.data.TensorDataset(X_train_scaled_tensor, y_train_tensor)\n",
    "# datasets_scaled = torch.utils.data.TensorDataset(X_train_scaled_tensor, y_train_scaled_tensor)\n",
    "train_iter_scaled = torch.utils.data.DataLoader(datasets_scaled, batch_size=10, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model_scaled = myModel()\n",
    "model_scaled.train()\n",
    "model_scaled = train(model_scaled, train_iter_scaled, num_epochs)\n",
    "\n",
    "# Evaluate model\n",
    "model_scaled.eval()\n",
    "outputs = model_scaled(X_test_scaled_tensor)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test_tensor.detach().numpy()))\n",
    "# err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test_scaled_tensor.detach().numpy()))\n",
    "\n",
    "print('model err: ', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "torch.save(model_scaled, 'scaled_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_5000_scaled = torch.empty((5000, 6), dtype=torch.float32)\n",
    "# for idx in range(5000):\n",
    "# \ti = random.randint(0, X_test_scaled_tensor.shape[0]-1)\n",
    "# \tX_test_5000_scaled[idx] = X_test_scaled_tensor[i]\n",
    "\n",
    "X_test_5000_scaled = X_test_scaled_tensor[:1000]\n",
    "\n",
    "sal_scaled = saliency(model_scaled, X_test_5000_scaled)\n",
    "ixg_scaled = input_x_gradient(model_scaled, X_test_5000_scaled)\n",
    "ig_scaled = integrated_gradients(model_scaled, X_test_5000_scaled)\n",
    "gradshap_scaled = gradient_shap(model_scaled, X_test_5000_scaled, X_train_scaled_tensor)\n",
    "fa_scaled = feature_ablation(model_scaled, X_test_5000_scaled)\n",
    "\n",
    "attr_test_norm_sum_list_scaled = []\n",
    "for attr_test in [sal_scaled, ixg_scaled, ig_scaled, gradshap_scaled, fa_scaled]:\n",
    "\tattr_test_norm_sum_list_scaled.append(attr_test_norm_sum(attr_test))\n",
    "\n",
    "y_axis_lin_weight_scaled = get_y_axis_lin_weight(model_scaled)\n",
    "\n",
    "plot(X_test_5000_scaled, attr_test_norm_sum_list_scaled, y_axis_lin_weight_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'Scaled': [128, 256, 512, 1024],\n",
    "                   'Non-scaled': [training_times[128], training_times[256], training_times[512], training_times[1024]]\n",
    "                  })\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of GradientSHAP attributions\n",
    "|   | Non-scaled | Scaled |\n",
    "| --- | --- | --- |\n",
    "| dist_to_nearest_stn | -0.17 | -0.12 |\n",
    "| dist_to_dhoby | 0.62 | 0.03 |\n",
    "| degree_centrality | 0 | 0.06\n",
    "| eigenvector_centrality | 0 | 0\n",
    "| remaining_lease_years | -0.08 | 0.65\n",
    "| floor_area_sqm | -0.14 | -0.1\n",
    "\n",
    "In the model trained and tested on <ins>scaled</ins> data, GradientSHAP indicates that <ins>*remaining_lease_years*</ins> is the <ins>most important feature</ins> among the 6 continuous features, scoring an attribution of nearly 0.65. However, in the model trained on <ins>un-scaled</ins> data, GradientSHAP indicates that <ins>*dist-to_dhoby*</ins> is the <ins>most important</ins> feature, scoring approximately 0.62, whereas *remaining_lease_years* only scored approximately -0.08. <br>\n",
    "Additionally, for the feature <ins>*remaining_lease_years*</ins>, in the model trained and tested on <ins>scaled</ins> data, it shows a <ins>positive attribution of approximately 0.65</ins>, but for the <ins>un-scaled</ins> case, it showed a <ins>negative attribution of approximately -0.08</ins>\n",
    "<br> <br>\n",
    "This is because GradientSHAP computations are based on the baseline, which is X_train and X_train_scaled in our two models respectively. When the data is scaled, it reduces the impact of the outlier data points on the model trained and hence results in different GradientSHAP attributions in the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX9iqK6SmhQ5"
   },
   "source": [
    "Read https://distill.pub/2020/attribution-baselines/ to build up your understanding of Integrated Gradients (IG). Reading the sections before the section on ‘Game Theory and Missingness’ will be sufficient. Keep in mind that this article mainly focuses on classification problems. You might find the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum useful as well.\n",
    "\n",
    "\n",
    "Then, answer the following questions in the context of our dataset:\n",
    "\n",
    "> Why did Saliency produce scores similar to IG?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67WqoEltmlRb"
   },
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "In both the scaled and non-scaled cases, Saliency did not produce scores similar to IG, especially in the non-scaled case where the score for Saliency is positive while the score for IG is negative.\n",
    "<br>\n",
    "This is because IG score is obtained after a multiplication with the input, which is absent in Saliency. Hence, depening on the input value, the IG score can be very different from the Saliency score. \n",
    "Additionally, IG also takes into account alpha. It considers the integral of (alpha multipied by the gradient of the output with respect to the input) over alpha, while Saliency only takes the gradient of output with respect to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYpfn3nCml1K"
   },
   "source": [
    "> Why did Input x Gradients give the same attribution scores as IG?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5OmMEdMmnP_"
   },
   "source": [
    "\\# TODO: \\<Enter your answer here\\>\n",
    "\n",
    "Input x Gradients give attribution scores that are almost completely identical as IG (only extremely small differences can be spotted for features *dist_to_dhoby* and *floor_area_sqm). <br>\n",
    "This is because in our context here, both Input x Gradients and IG use the zero-vector as their baselines. <br>\n",
    "Hence, the computation for IG reduces to the product of the input and the integral of (alpha multipied by the gradient of the output with respect to the input) over alpha.\n",
    "Thus, Input x Gradients and IG give almost identical attribution scores"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
