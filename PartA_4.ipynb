{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "987c7c95a0c7dc71b3d85e154cc3a9be",
     "grade": false,
     "grade_id": "cell-6ebb8bd2f22353d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f824c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5c8f824c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17d770ae590711dc06f03d150970a3f1",
     "grade": false,
     "grade_id": "cell-e34b0415c38ebac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, we will understand the utility of such a neural network in real world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "287259c58079728b66dae175c6082400",
     "grade": false,
     "grade_id": "cell-4f74b97314b65ea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Please use the real record data named ‘record.wav’  as a test sample. Preprocess the data using the provided preprocessing script (data_preprocess.ipynb) and prepare the dataset.\n",
    "Do a model prediction on the sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons.\n",
    "Find the most important features on the model prediction for the test sample using SHAP. Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5,\n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c85ca-9a14-4d0a-b44d-814f02c0f8e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "981c85ca-9a14-4d0a-b44d-814f02c0f8e1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30c3b93836aad148380e15933e7dd786",
     "grade": false,
     "grade_id": "cell-b8a265bf37e3b271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Firstly, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c50f4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "58c50f4f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f6af6091e2832c850b00e735d1cff11",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3444c83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d3444c83",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d796a3a33dd56bd5afb55de45b642449",
     "grade": false,
     "grade_id": "cell-293c9e85ad81d29a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To reduce repeated code, place your\n",
    "network (MLP defined in QA1)\n",
    "torch datasets (CustomDataset defined in QA1)\n",
    "loss function (loss_fn defined in QA1)\n",
    "in a separate file called common_utils.py\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8e840",
   "metadata": {
    "deletable": false,
    "id": "72e8e840",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c623c0417cb6065d1bbb049f211cf1c",
     "grade": false,
     "grade_id": "cell-29dace0045a28b89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import MLP, CustomDataset, loss_fn, preprocess_dataset, split_dataset, EarlyStopper\n",
    "\n",
    "optimal_batch_size = 256\n",
    "optimal_neurons = 128\n",
    "\n",
    "def train(model, X_train_scaled, y_train2, X_val_scaled, y_val2, batch_size=optimal_batch_size):\n",
    "    # YOUR CODE HERE\n",
    "    train_accuracies, train_losses, test_accuracies, test_losses, times = [], [], [], [], []\n",
    "    # each list contains the accuracy/loss/time for each epoch \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    early_stopper = EarlyStopper()\n",
    "    training_data = CustomDataset(X_train_scaled, y_train2)\n",
    "    training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    validation_data = CustomDataset(X_val_scaled, y_val2)\n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "    no_epochs = 100\n",
    "    for epoch in range(no_epochs):\n",
    "        train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "        # Training\n",
    "        start_time = time.time()\n",
    "        for i, (x, y) in enumerate(training_dataloader):\n",
    "            # Prediction\n",
    "            y_pred = model(x) # shape: [256, 1]\n",
    "            y_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "            # Compute loss and accuracy\n",
    "            loss = loss_fn(y_pred.float(), y.float())\n",
    "            train_loss += loss.item()\n",
    "            # transform y_pred to give class 0 or class 1\n",
    "            pred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "            # compare pred_label with the ground truth y, add 1 to the train_acc variable if the prediction is correct\n",
    "            train_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        training_time = time.time() - start_time\n",
    "        times.append(training_time)\n",
    "        train_losses.append(train_loss/ len(training_dataloader))\n",
    "        train_accuracies.append(train_acc/ len(training_dataloader.dataset))\n",
    "        # Testing\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(validation_dataloader):\n",
    "                # Prediction\n",
    "                y_pred = model(x) # shape: [256, 1]\n",
    "                y_pred = y_pred.squeeze(dim=1) # to get shape [256]\n",
    "                # Compute loss and accuracy\n",
    "                loss = loss_fn(y_pred.float(), y.float())\n",
    "                test_loss += loss.item()\n",
    "                # transform y_pred to give class 0 or class 1\n",
    "                pred_label = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "                # compare pred_label with the ground truth y, add 1 to the test_acc variable if the prediction is correct\n",
    "                test_acc += sum([1 if i == j else 0 for i, j in zip(pred_label, y)])\n",
    "        test_losses.append(test_loss/ len(validation_dataloader))\n",
    "        test_accuracies.append(test_acc/ len(validation_dataloader.dataset))\n",
    "        if early_stopper.early_stop(test_loss):\n",
    "            print(\"Early stopping at epoch number: \", epoch+1)\n",
    "            break\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f3ced-a6a1-4628-a409-1ca7bdfd1cfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b12f3ced-a6a1-4628-a409-1ca7bdfd1cfa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbab024c3394801484199efdbbdb269",
     "grade": true,
     "grade_id": "corrected",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('simplified.csv')\n",
    "train_df['label'] = train_df['filename'].str.split('_').str[-2]\n",
    "df_train, y_train, df_val, y_val = split_dataset(train_df, ['filename'], 0.3, 42)\n",
    "X_train = df_train.drop(columns=[\"label\"])\n",
    "X_val = df_val.drop(columns=[\"label\"])\n",
    "# Scale the training and validation input features\n",
    "X_train_scaled, X_val_scaled = preprocess_dataset(X_train, X_val)\n",
    "\n",
    "model = MLP(no_hidden=optimal_neurons, no_features=77, no_labels=1)\n",
    "model = train(model, X_train_scaled, y_train, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd5d5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "18fd5d5e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7da5539e4fe97549a11c7d61be647167",
     "grade": false,
     "grade_id": "cell-1c5bf554b8f89a3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Install and import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49be1fc",
   "metadata": {
    "deletable": false,
    "id": "e49be1fc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58a0104d88201d0af7de9fc3a6ca035",
     "grade": false,
     "grade_id": "import_shap",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef497933-2108-4aa5-8ec8-5729214cb1cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ef497933-2108-4aa5-8ec8-5729214cb1cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cf4df5a01325e8ea1f585dcfc81b01b",
     "grade": true,
     "grade_id": "import_shap_correct",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fde60a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5fde60a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8877105a451813ab23b45e9a180bc36",
     "grade": false,
     "grade_id": "cell-82dd5a271bf5af4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Read the csv data preprocessed from 'record.wav', using variable name 'df', and fill the size of 'df' in 'size_row' and 'size_column'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a54d47",
   "metadata": {
    "deletable": false,
    "id": "81a54d47",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c35348846173e5c042d78be10546ae86",
     "grade": false,
     "grade_id": "cell-01d5f7ef70e69e09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = 0\n",
    "size_row = 0\n",
    "size_column = 0\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_csv('new_record.csv')\n",
    "size_row = df.shape[0]\n",
    "size_column = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b0b06-1750-4228-88af-67d8c52035dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "571b0b06-1750-4228-88af-67d8c52035dc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d137f7e21ec2ea9ad7a57f4411b513a",
     "grade": true,
     "grade_id": "cell-01d5f7ef70e69e0988",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3d13eea6f0ed0d345e10f33dd3a26da",
     "grade": false,
     "grade_id": "cell-7096e580d10284df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " 4.  Preprocess to obtain the test data, save the test data as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77bd18-c546-473e-8c2f-643b4281d9ba",
   "metadata": {
    "deletable": false,
    "id": "8c77bd18-c546-473e-8c2f-643b4281d9ba",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b19be33055efd5fc5d562a9c671b6eb2",
     "grade": false,
     "grade_id": "cell-b1e392e8ecab207a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(X_train, df):\n",
    "    \"\"\"preprocess your dataset to obtain your test dataset, remember to remove the 'filename' as Q1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Remove 'filename' attribute from dataframe since model was trained without this attribute\n",
    "    df2 = df.drop(['filename'],axis=1)\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    # Fit the scaler to the X_train data\n",
    "    standard_scaler = standard_scaler.fit(X_train)\n",
    "    # Transform the test data using the fitted scaler\n",
    "    X_test_scaled_eg = standard_scaler.transform(df2)\n",
    "    return X_test_scaled_eg\n",
    "\n",
    "X_test_scaled_eg = preprocess(X_train, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ee8a7-d9b2-499d-8394-d6cb86f4cb60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e49ee8a7-d9b2-499d-8394-d6cb86f4cb60",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0c0b2a92c7d501f1ac652e11f948461",
     "grade": true,
     "grade_id": "cell-fbe8ba077fb74598",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96d2e019d65c49ba15b3089c2184f021",
     "grade": false,
     "grade_id": "cell-48b4edbfec330f39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Do a model prediction on the sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. Note: Please define the variable of your final predicted label as 'pred_label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "276ec9575db4ca701823a459809ea810",
     "grade": true,
     "grade_id": "cell-e83cb49660edc2b7",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_test_scaled_tensor = torch.tensor(X_test_scaled_eg, dtype=torch.float32)\n",
    "pred = model(X_test_scaled_tensor)\n",
    "print(pred)\n",
    "threshold = 0.5\n",
    "if pred.item() > threshold:\n",
    "\tpred_label = 1\n",
    "else:\n",
    "\tpred_label = 0\n",
    "print(\"The prediction (pred_label) is: \", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fc2cc-b89f-4fc3-af16-e30b4e5315a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "da2fc2cc-b89f-4fc3-af16-e30b4e5315a3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "704df2be8fbd85ba163a89cd2e0431f0",
     "grade": true,
     "grade_id": "predict_value",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eac3438866b5ebd40f5fb20a676059bd",
     "grade": false,
     "grade_id": "cell-896f18b6b0b948ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Find the most important features on the model prediction for your test sample using SHAP. Create an instance of the DeepSHAP which is called DeepExplainer using traianing dataset: https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html.\n",
    "\n",
    "Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5,\n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15591e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db46b1b26fd45359768421987104ac3e",
     "grade": true,
     "grade_id": "importance_weight",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fit the explainer on a subset of the data (you can try all but then gets slower)\n",
    "Return approximate SHAP values for the model applied to the data given by X.\n",
    "Plot the local feature importance with a force plot and explain your observations.\n",
    "'''\n",
    "# YOUR CODE HERE\n",
    "training_data = CustomDataset(X_train_scaled, y_train)\n",
    "training_dataloader = DataLoader(training_data, batch_size=optimal_batch_size, shuffle=True)\n",
    "batch = next(iter(training_dataloader))\n",
    "images, _ = batch\n",
    "background = images[:1000]\n",
    "\n",
    "test_image = X_test_scaled_tensor\n",
    "\n",
    "# I use the first 1000 training examples as the background dataset to integrate over\n",
    "e = shap.DeepExplainer(model, background)\n",
    "\n",
    "# Explain the test prediction\n",
    "shap_values = e.shap_values(test_image) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(e.expected_value, shap_values, features = X_test_scaled_eg, feature_names=list(df.drop(['filename'],axis=1).columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation\n",
    "\"\"\"\n",
    "From the force plot above, we see that the prediction label assigned by the model to a test sample largely depends on features spec_bw_var, cent_var, mfcc2_var and constrast_mean. Specifically, features spec_bw_var and contrast_mean would be the two main features pushing the prediction towards class \"1\", while features cent_var and mfcc_var would be the two main features that pulls the prediction away from class \"1\" ie. pushing the prediction towards class \"0\".\n",
    "So this particular recording was ultimately classified as \"1\", because it is pushed more to the right by all the factors shown in red\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
